{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiltWCZEXz6e"
      },
      "source": [
        "\n",
        "# Inertia as a Form of Model Compression in Convolutional Neural Networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvzprg3LjNXa"
      },
      "source": [
        "> *In submission for Deep Learning Project, Spring 2025*\n",
        "\n",
        "**Labiba Shahab**\n",
        "\n",
        "**Ahmed Wali**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDVIU3SPZwCS"
      },
      "source": [
        "\n",
        "> *What you usually achieve with a standard convolution, you can accomplish efficiently using a smaller convolution + peripheral inertia mechanism (inertial filter)*\n",
        "~ Group 39\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en5sIuGzQmyM"
      },
      "source": [
        "## In a Gist:\n",
        "\n",
        "Standard dxd convolution layers involve d^2 learnable parameters and d^2 computations per convolution operation. In this project, we propose an inertial convolution mechanism that dynamically decides whether a detailed convolution is necessary. The goal is to reduce both computations and learnable parameters while maintaining performance on vision tasks like MNIST classification.\n",
        "\n",
        "Instead of learning a full dxd kernel, we use a (d-k)x(d-k) core filter to convolve a central patch. The surrounding d^2-(d-k)^2 pixels act as an inertial periphery—evaluating local divergence or “friction.” If the divergence is high, we re-apply the core filter across the full dxd region in another convolution and stack the outputs. If low, we skip detailed computation.\n",
        "\n",
        "In the best case, we perform just d-k computation with d-k learnable parameters.\n",
        "In the worst case, we perform up to d computations, but still only learn d-k parameters, hence effectively pruning the model with estimation compression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i47fZ7RatOZ"
      },
      "source": [
        "## Background\n",
        "\n",
        "There has been extensive research in reducing neural network complexity:\n",
        "\n",
        "Dynamic convolutions and skip-convolutions conditionally skip expensive operations. This project is inspired from [Dynamic Sparse Convolutions](https://arxiv.org/pdf/2102.04906), [Skip Convolutions](https://openaccess.thecvf.com/content/CVPR2021/papers/Habibian_Skip-Convolutions_for_Efficient_Video_Processing_CVPR_2021_paper.pdf), and [Fractional Skipping](https://arxiv.org/abs/2001.00705)\n",
        "\n",
        "Pruning, quantization, and knowledge distillation reduce parameters, memory, or model depth.\n",
        "\n",
        "Our approach is inspired by these ideas but focuses on parameter reuse and friction-aware skipping, offering a novel trade-off between learning capacity and computational efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg1MOaikbd1b"
      },
      "source": [
        "#### Novelty\n",
        "\n",
        "Inspired from computational optimization, our project proposes a similar mechanism to do model compression by estimation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTrN-SAKcybU"
      },
      "source": [
        "## Baseline Models\n",
        "\n",
        "#### Fashion MNIST\n",
        "The baseline CNN architecture is adapted from the official [Fashion-MNIST GitHub benchmark](https://github.com/zalandoresearch/fashion-mnist) implementation in TensorFlow. We replicated the structure and hyperparameters in PyTorch for fair benchmarking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rES1zu0QmyN"
      },
      "source": [
        "Reproducibility + Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-14T09:49:04.354693Z",
          "iopub.status.busy": "2025-04-14T09:49:04.354461Z",
          "iopub.status.idle": "2025-04-14T09:49:13.613774Z",
          "shell.execute_reply": "2025-04-14T09:49:13.613188Z",
          "shell.execute_reply.started": "2025-04-14T09:49:04.354669Z"
        },
        "id": "_4fFqm_HQmyO",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujIz2djQmyP"
      },
      "source": [
        "Seeds and CuDNN configs for deterministic results on a GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-14T09:49:13.615644Z",
          "iopub.status.busy": "2025-04-14T09:49:13.615269Z",
          "iopub.status.idle": "2025-04-14T09:49:13.675190Z",
          "shell.execute_reply": "2025-04-14T09:49:13.674442Z",
          "shell.execute_reply.started": "2025-04-14T09:49:13.615626Z"
        },
        "id": "CIye0712QmyP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxGB9gtuQmyQ"
      },
      "source": [
        "Load Fashion MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-04-14T09:49:13.678190Z",
          "iopub.status.busy": "2025-04-14T09:49:13.677920Z",
          "iopub.status.idle": "2025-04-14T09:49:17.693083Z",
          "shell.execute_reply": "2025-04-14T09:49:17.692347Z",
          "shell.execute_reply.started": "2025-04-14T09:49:13.678164Z"
        },
        "id": "32-TJ2GrQmyQ",
        "outputId": "8526a1c7-15b3-4b02-c963-cc55ebf4a2df",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 10.1MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 164kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.19MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 22.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.FashionMNIST('./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=400, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSS5g77IQmyR"
      },
      "source": [
        "Test Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-14T09:49:17.694403Z",
          "iopub.status.busy": "2025-04-14T09:49:17.694118Z",
          "iopub.status.idle": "2025-04-14T09:49:17.699203Z",
          "shell.execute_reply": "2025-04-14T09:49:17.698449Z",
          "shell.execute_reply.started": "2025-04-14T09:49:17.694384Z"
        },
        "id": "G-tjePwcQmyR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    accuracy = correct / len(test_loader.dataset)\n",
        "    print(f\"Accuracy: {accuracy:.10f}\")\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlkggp0RQmyS"
      },
      "source": [
        "#### Fashion MNIST 5x5 ConvNet Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-14T09:49:17.700071Z",
          "iopub.status.busy": "2025-04-14T09:49:17.699900Z",
          "iopub.status.idle": "2025-04-14T09:49:17.717488Z",
          "shell.execute_reply": "2025-04-14T09:49:17.716801Z",
          "shell.execute_reply.started": "2025-04-14T09:49:17.700057Z"
        },
        "id": "tynoqrTGQmyS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@originally adapted from https://github.com/zalandoresearch/fashion-mnist/blob/master/benchmark/convnet.py\n",
        "class FashionNet5x5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 5, padding=2)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 5, padding=2)\n",
        "        self.fc1 = nn.Linear(7*7*64, 1024)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.fc2 = nn.Linear(1024, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sii_UIaGQmyT"
      },
      "source": [
        "#### Fashion MNIST 3x3 ConvNet Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MZpjFDJUQmyT"
      },
      "outputs": [],
      "source": [
        "# @adaptation of the above 5x5 model to use 3x3 convolutions\n",
        "class FashionNet3x3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 1024)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.fc2 = nn.Linear(1024, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFBgio7bQmyT"
      },
      "source": [
        "#### Adapted Fashion MNIST 1x1 ConvNet Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oatPdaOMQmyU"
      },
      "outputs": [],
      "source": [
        "#@adaptation of the above 5x5 model to use 1x1 convolutions\n",
        "class FashionNet1x1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 1)\n",
        "        self.fc1 = nn.Linear(7*7*64, 1024)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.fc2 = nn.Linear(1024, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUihOL46QmyU"
      },
      "source": [
        "Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-14T09:49:17.737971Z",
          "iopub.status.busy": "2025-04-14T09:49:17.737739Z",
          "iopub.status.idle": "2025-04-14T09:49:17.752558Z",
          "shell.execute_reply": "2025-04-14T09:49:17.751933Z",
          "shell.execute_reply.started": "2025-04-14T09:49:17.737952Z"
        },
        "id": "MH8OgsujQmyU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train_model(model, name):\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "    epochs = 100\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = F.cross_entropy(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    print(f\"\\n{name} Results:\")\n",
        "    acc = test(model, device, test_loader)\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sIIBO11QmyV"
      },
      "source": [
        "So Let's Evaluate Fashion MNIST on the Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-04-14T09:57:48.734046Z",
          "iopub.status.busy": "2025-04-14T09:57:48.733317Z",
          "iopub.status.idle": "2025-04-14T10:03:18.108237Z",
          "shell.execute_reply": "2025-04-14T10:03:18.107591Z",
          "shell.execute_reply.started": "2025-04-14T09:57:48.734020Z"
        },
        "id": "fbYf3xWvQmyV",
        "outputId": "5ce085c2-4867-400a-95dd-a2af640ea6a7",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "FashionNet 5x5 Results:\n",
            "Accuracy: 0.8338000000\n",
            "\n",
            "FashionNet 3x3 Results:\n",
            "Accuracy: 0.8317000000\n",
            "\n",
            "FashionNet 1x1 Results:\n",
            "Accuracy: 0.8169000000\n"
          ]
        }
      ],
      "source": [
        "model_5x5 = FashionNet5x5()\n",
        "acc_5x5 = train_model(model_5x5, \"FashionNet 5x5\")\n",
        "\n",
        "model_3x3 = FashionNet3x3()\n",
        "acc_3x3 = train_model(model_3x3, \"FashionNet 3x3\")\n",
        "\n",
        "model_1x1 = FashionNet1x1()\n",
        "acc_1x1 = train_model(model_1x1, \"FashionNet 1x1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJVGWsL_QmyW"
      },
      "source": [
        "#### Note:\n",
        "\n",
        "Fashion-MNIST is a drop-in replacement for MNIST but contains Zalando's article images.\n",
        "- **Existing SOA Model**: Simple CNN / ResNet-based\n",
        "- **Expected Accuracy**: ~93-95%\n",
        "- **Link**: [Fashion-MNIST GitHub](https://github.com/zalandoresearch/fashion-mnist)\n",
        "\n",
        "This gives a little more divergence, hence giving us with a more naunced understanding of the application of the inertial filters."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31011,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
