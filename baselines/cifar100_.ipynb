{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# Inertia as a Form of Model Compression in Convolutional Neural Networks\n","metadata":{"id":"LiltWCZEXz6e"}},{"cell_type":"markdown","source":"> *In submission for Deep Learning Project, Spring 2025*\n\n**Labiba Shahab**\n\n**Ahmed Wali**","metadata":{"id":"Nvzprg3LjNXa"}},{"cell_type":"markdown","source":"\n> *What you usually achieve with a standard convolution, you can accomplish efficiently using a smaller convolution + peripheral inertia mechanism (inertial filter)*\n~ Group 39\n","metadata":{"id":"cDVIU3SPZwCS"}},{"cell_type":"markdown","source":"## In a Gist:\n\nStandard dxd convolution layers involve d^2 learnable parameters and d^2 computations per convolution operation. In this project, we propose an inertial convolution mechanism that dynamically decides whether a detailed convolution is necessary. The goal is to reduce both computations and learnable parameters while maintaining performance on vision tasks like MNIST classification.\n\nInstead of learning a full dxd kernel, we use a (d-k)x(d-k) core filter to convolve a central patch. The surrounding d^2-(d-k)^2 pixels act as an inertial periphery—evaluating local divergence or “friction.” If the divergence is high, we re-apply the core filter across the full dxd region in another convolution and stack the outputs. If low, we skip detailed computation.\n\nIn the best case, we perform just d-k computation with d-k learnable parameters.\nIn the worst case, we perform up to d computations, but still only learn d-k parameters, hence effectively pruning the model with estimation compression.","metadata":{}},{"cell_type":"markdown","source":"## Background\n\nThere has been extensive research in reducing neural network complexity:\n\nDynamic convolutions and skip-convolutions conditionally skip expensive operations. This project is inspired from [Dynamic Sparse Convolutions](https://arxiv.org/pdf/2102.04906), [Skip Convolutions](https://openaccess.thecvf.com/content/CVPR2021/papers/Habibian_Skip-Convolutions_for_Efficient_Video_Processing_CVPR_2021_paper.pdf), and [Fractional Skipping](https://arxiv.org/abs/2001.00705)\n\nPruning, quantization, and knowledge distillation reduce parameters, memory, or model depth.\n\nOur approach is inspired by these ideas but focuses on parameter reuse and friction-aware skipping, offering a novel trade-off between learning capacity and computational efficiency.","metadata":{"id":"-i47fZ7RatOZ"}},{"cell_type":"markdown","source":"#### Novelty\n\nInspired from computational optimization, our project proposes a similar mechanism to do model compression by estimation.","metadata":{"id":"Eg1MOaikbd1b"}},{"cell_type":"markdown","source":"## Baseline Models\n\n#### CIFAR \nIn this dataset, we needed models model that are simple enough to modify, high-performing enough to be credible and reproducible, and modular enough to swap out Conv2d layers to test our inertial convolution. We chose ResNet18, VGG16, and SimpleDLA as the baselines models on the CIFAR-10 dataset directly adapted from [kuangliu/pytorch-cifar](https://github.com/kuangliu/pytorch-cifar). ","metadata":{"id":"bTrN-SAKcybU"}},{"cell_type":"markdown","source":"Reproducibility + Imports","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\nimport random\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2025-04-14T16:55:13.790376Z","iopub.execute_input":"2025-04-14T16:55:13.791054Z","iopub.status.idle":"2025-04-14T16:55:21.981277Z","shell.execute_reply.started":"2025-04-14T16:55:13.791026Z","shell.execute_reply":"2025-04-14T16:55:21.980458Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"Seeds and CuDNN configs for deterministic results on a GPU","metadata":{}},{"cell_type":"code","source":"seed = 1\ntorch.manual_seed(seed)\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2025-04-14T16:55:21.982037Z","iopub.execute_input":"2025-04-14T16:55:21.982406Z","iopub.status.idle":"2025-04-14T16:55:22.042866Z","shell.execute_reply.started":"2025-04-14T16:55:21.982381Z","shell.execute_reply":"2025-04-14T16:55:22.042121Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Load CIFAR-10 Dataset","metadata":{}},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntrain_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=transform)\ntest_dataset = datasets.CIFAR10('./data', train=False, download=True, transform=transform)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2025-04-14T16:55:22.044483Z","iopub.execute_input":"2025-04-14T16:55:22.044715Z","iopub.status.idle":"2025-04-14T16:55:40.753712Z","shell.execute_reply.started":"2025-04-14T16:55:22.044698Z","shell.execute_reply":"2025-04-14T16:55:40.753179Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170M/170M [00:14<00:00, 12.2MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Test Function","metadata":{}},{"cell_type":"code","source":"def test(model, device, test_loader):\n    model.eval()\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n    accuracy = correct / len(test_loader.dataset)\n    print(f\"Accuracy: {accuracy:.10f}\")\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2025-04-14T16:55:40.754317Z","iopub.execute_input":"2025-04-14T16:55:40.754524Z","iopub.status.idle":"2025-04-14T16:55:40.759335Z","shell.execute_reply.started":"2025-04-14T16:55:40.754507Z","shell.execute_reply":"2025-04-14T16:55:40.758569Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Baseline Architectures\n\nThe following blocks will define the 4 baseline models adapted from KuangLiu's repo.\n- To integrate inertial filters, we later plan to replace `nn.Conv2d` with `InertialConv2d` (which we would define in future) in these blocks.","metadata":{}},{"cell_type":"markdown","source":"ResNet18 Model","metadata":{}},{"cell_type":"code","source":"#@adapted from https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py\nclass BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, in_planes, planes, stride=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, 3, stride, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, 3, 1, 1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion * planes, 1, stride, bias=False),\n                nn.BatchNorm2d(self.expansion * planes)\n            )\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        return F.relu(out)\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super().__init__()\n        self.in_planes = 64\n        self.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64,  num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512 * block.expansion, num_classes)\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for s in strides:\n            layers.append(block(self.in_planes, planes, s))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        return self.linear(out)\n\ndef ResNet18():\n    return ResNet(BasicBlock, [2, 2, 2, 2])","metadata":{"execution":{"iopub.status.busy":"2025-04-14T16:55:40.760058Z","iopub.execute_input":"2025-04-14T16:55:40.760695Z","iopub.status.idle":"2025-04-14T16:55:40.773697Z","shell.execute_reply.started":"2025-04-14T16:55:40.760674Z","shell.execute_reply":"2025-04-14T16:55:40.773117Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"VGG-16 Model","metadata":{}},{"cell_type":"code","source":"#@adapted from https://github.com/kuangliu/pytorch-cifar/blob/master/models/vgg.py\ncfg = {\n    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M',\n              512, 512, 512, 'M', 512, 512, 512, 'M'],\n}\n\nclass VGG(nn.Module):\n    def __init__(self, vgg_name='VGG16'):\n        super().__init__()\n        self.features = self._make_layers(cfg[vgg_name])\n        self.classifier = nn.Linear(512, 10)\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        return self.classifier(x)\n    def _make_layers(self, cfg):\n        layers = []\n        in_channels = 3\n        for x in cfg:\n            if x == 'M':\n                layers += [nn.MaxPool2d(2, 2)]\n            else:\n                layers += [\n                    nn.Conv2d(in_channels, x, 3, padding=1),\n                    nn.BatchNorm2d(x),\n                    nn.ReLU(inplace=True)\n                ]\n                in_channels = x\n        layers += [nn.AvgPool2d(1)]\n        return nn.Sequential(*layers)","metadata":{"execution":{"iopub.status.busy":"2025-04-14T16:55:40.774251Z","iopub.execute_input":"2025-04-14T16:55:40.774436Z","iopub.status.idle":"2025-04-14T16:55:40.785156Z","shell.execute_reply.started":"2025-04-14T16:55:40.774422Z","shell.execute_reply":"2025-04-14T16:55:40.784588Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"SimpleDLA Model","metadata":{}},{"cell_type":"code","source":"#@adapted from https://github.com/kuangliu/pytorch-cifar/blob/master/models/dla_simple.py\nclass Root(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=1):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, 1,\n                              padding=(kernel_size - 1) // 2, bias=False)\n        self.bn = nn.BatchNorm2d(out_channels)\n    def forward(self, xs):\n        x = torch.cat(xs, 1)\n        return F.relu(self.bn(self.conv(x)))\n\nclass Tree(nn.Module):\n    def __init__(self, block, in_channels, out_channels, level=1, stride=1):\n        super().__init__()\n        self.root = Root(2*out_channels, out_channels)\n        if level == 1:\n            self.left = block(in_channels, out_channels, stride)\n            self.right = block(out_channels, out_channels)\n        else:\n            self.left = Tree(block, in_channels, out_channels, level-1, stride)\n            self.right = Tree(block, out_channels, out_channels, level-1, 1)\n    def forward(self, x):\n        x1 = self.left(x)\n        x2 = self.right(x1)\n        return self.root([x1, x2])\n\nclass SimpleDLA(nn.Module):\n    def __init__(self, block=BasicBlock, num_classes=10):\n        super().__init__()\n        self.base = nn.Sequential(\n            nn.Conv2d(3, 16, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(16),\n            nn.ReLU(inplace=True)\n        )\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(16, 16, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(16),\n            nn.ReLU(inplace=True)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 32, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True)\n        )\n        self.layer3 = Tree(block, 32, 64, level=1, stride=1)\n        self.layer4 = Tree(block, 64, 128, level=2, stride=2)\n        self.layer5 = Tree(block, 128, 256, level=2, stride=2)\n        self.layer6 = Tree(block, 256, 512, level=1, stride=2)\n        self.linear = nn.Linear(512, num_classes)\n    def forward(self, x):\n        x = self.base(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        x = self.layer6(x)\n        x = F.avg_pool2d(x, 4)\n        x = x.view(x.size(0), -1)\n        return self.linear(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:55:40.785887Z","iopub.execute_input":"2025-04-14T16:55:40.786337Z","iopub.status.idle":"2025-04-14T16:55:40.800687Z","shell.execute_reply.started":"2025-04-14T16:55:40.786313Z","shell.execute_reply":"2025-04-14T16:55:40.800166Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"Training Function","metadata":{}},{"cell_type":"code","source":"def train_model(model, name):\n    model = model.to(device)\n    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 225], gamma=0.1)\n    epochs = 200\n\n    for epoch in range(epochs):\n        model.train()\n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.cross_entropy(output, target)\n            loss.backward()\n            optimizer.step()\n        scheduler.step()\n        print(f\"Epoch {epoch} complete.\")\n\n    print(f\"\\\\n{name} Results:\")\n    acc = test(model, device, test_loader)\n    return acc","metadata":{"execution":{"iopub.status.busy":"2025-04-14T16:55:40.801412Z","iopub.execute_input":"2025-04-14T16:55:40.801856Z","iopub.status.idle":"2025-04-14T16:55:40.814078Z","shell.execute_reply.started":"2025-04-14T16:55:40.801837Z","shell.execute_reply":"2025-04-14T16:55:40.813452Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"So Let's Evaluate MNIST on the Baseline","metadata":{}},{"cell_type":"code","source":"model_rn18 = ResNet18()\nacc_rn18 = train_model(model_rn18, \"ResNet18\")\n\nmodel_vgg16 = VGG('VGG16')\nacc_vgg16 = train_model(model_vgg16, \"VGG16\")\n\nmodel_simpledla = SimpleDLA()\nacc_simpledla = train_model(model_simpledla, \"SimpleDLA\")","metadata":{"execution":{"iopub.status.busy":"2025-04-14T16:55:40.815962Z","iopub.execute_input":"2025-04-14T16:55:40.816503Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 0 complete.\nEpoch 1 complete.\nEpoch 2 complete.\nEpoch 3 complete.\nEpoch 4 complete.\nEpoch 5 complete.\nEpoch 6 complete.\nEpoch 7 complete.\nEpoch 8 complete.\nEpoch 9 complete.\nEpoch 10 complete.\nEpoch 11 complete.\nEpoch 12 complete.\nEpoch 13 complete.\nEpoch 14 complete.\nEpoch 15 complete.\nEpoch 16 complete.\nEpoch 17 complete.\nEpoch 18 complete.\nEpoch 19 complete.\nEpoch 20 complete.\nEpoch 21 complete.\nEpoch 22 complete.\nEpoch 23 complete.\nEpoch 24 complete.\nEpoch 25 complete.\nEpoch 26 complete.\nEpoch 27 complete.\nEpoch 28 complete.\nEpoch 29 complete.\nEpoch 30 complete.\nEpoch 31 complete.\nEpoch 32 complete.\nEpoch 33 complete.\nEpoch 34 complete.\nEpoch 35 complete.\nEpoch 36 complete.\nEpoch 37 complete.\nEpoch 38 complete.\nEpoch 39 complete.\nEpoch 40 complete.\nEpoch 41 complete.\nEpoch 42 complete.\nEpoch 43 complete.\nEpoch 44 complete.\nEpoch 45 complete.\nEpoch 46 complete.\nEpoch 47 complete.\nEpoch 48 complete.\nEpoch 49 complete.\nEpoch 50 complete.\nEpoch 51 complete.\nEpoch 52 complete.\nEpoch 53 complete.\nEpoch 54 complete.\nEpoch 55 complete.\nEpoch 56 complete.\nEpoch 56 complete.\nEpoch 57 complete.\nEpoch 58 complete.\nEpoch 59 complete.\nEpoch 60 complete.\nEpoch 61 complete.\nEpoch 62 complete.\nEpoch 63 complete.\nEpoch 64 complete.\nEpoch 65 complete.\nEpoch 66 complete.\nEpoch 67 complete.\nEpoch 68 complete.\nEpoch 69 complete.\nEpoch 70 complete.\nEpoch 71 complete.\nEpoch 72 complete.\nEpoch 73 complete.\nEpoch 74 complete.\nEpoch 75 complete.\nEpoch 76 complete.\nEpoch 77 complete.\nEpoch 78 complete.\nEpoch 79 complete.\nEpoch 80 complete.\nEpoch 81 complete.\nEpoch 82 complete.\nEpoch 83 complete.\nEpoch 84 complete.\nEpoch 85 complete.\nEpoch 86 complete.\nEpoch 87 complete.\nEpoch 88 complete.\nEpoch 89 complete.\nEpoch 90 complete.\nEpoch 91 complete.\nEpoch 92 complete.\nEpoch 93 complete.\nEpoch 94 complete.\nEpoch 95 complete.\nEpoch 96 complete.\nEpoch 97 complete.\nEpoch 98 complete.\nEpoch 99 complete.\nEpoch 100 complete.\nEpoch 101 complete.\nEpoch 102 complete.\nEpoch 103 complete.\nEpoch 104 complete.\nEpoch 105 complete.\nEpoch 106 complete.\nEpoch 107 complete.\nEpoch 108 complete.\nEpoch 109 complete.\nEpoch 110 complete.\nEpoch 111 complete.\nEpoch 112 complete.\nEpoch 113 complete.\nEpoch 114 complete.\nEpoch 115 complete.\nEpoch 116 complete.\nEpoch 117 complete.\nEpoch 118 complete.\nEpoch 119 complete.\nEpoch 120 complete.\nEpoch 121 complete.\nEpoch 122 complete.\nEpoch 123 complete.\nEpoch 124 complete.\nEpoch 125 complete.\nEpoch 126 complete.\nEpoch 127 complete.\nEpoch 128 complete.\nEpoch 129 complete.\nEpoch 130 complete.\nEpoch 131 complete.\nEpoch 132 complete.\nEpoch 133 complete.\nEpoch 134 complete.\nEpoch 135 complete.\nEpoch 136 complete.\nEpoch 137 complete.\nEpoch 138 complete.\nEpoch 139 complete.\nEpoch 140 complete.\nEpoch 141 complete.\nEpoch 142 complete.\nEpoch 143 complete.\nEpoch 144 complete.\nEpoch 145 complete.\nEpoch 146 complete.\nEpoch 147 complete.\nEpoch 148 complete.\nEpoch 149 complete.\nEpoch 150 complete.\nEpoch 151 complete.\nEpoch 152 complete.\nEpoch 153 complete.\nEpoch 154 complete.\nEpoch 155 complete.\nEpoch 156 complete.\nEpoch 157 complete.\nEpoch 158 complete.\nEpoch 159 complete.\nEpoch 160 complete.\nEpoch 161 complete.\nEpoch 162 complete.\nEpoch 163 complete.\nEpoch 164 complete.\nEpoch 165 complete.\nEpoch 166 complete.\nEpoch 167 complete.\nEpoch 168 complete.\nEpoch 169 complete.\nEpoch 170 complete.\nEpoch 171 complete.\nEpoch 172 complete.\nEpoch 173 complete.\nEpoch 174 complete.\nEpoch 175 complete.\nEpoch 176 complete.\nEpoch 177 complete.\nEpoch 178 complete.\nEpoch 179 complete.\nEpoch 180 complete.\nEpoch 181 complete.\nEpoch 182 complete.\nEpoch 183 complete.\nEpoch 184 complete.\nEpoch 185 complete.\nEpoch 186 complete.\nEpoch 187 complete.\nEpoch 188 complete.\nEpoch 189 complete.\nEpoch 190 complete.\nEpoch 191 complete.\nEpoch 192 complete.\nEpoch 193 complete.\nEpoch 194 complete.\nEpoch 195 complete.\nEpoch 196 complete.\nEpoch 197 complete.\nEpoch 198 complete.\nEpoch 199 complete.\n\\nResNet18 Results:\nAccuracy: 0.9248000000\nEpoch 0 complete.\nEpoch 1 complete.\nEpoch 2 complete.\nEpoch 3 complete.\nEpoch 4 complete.\nEpoch 5 complete.\nEpoch 6 complete.\nEpoch 7 complete.\nEpoch 8 complete.\nEpoch 9 complete.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"Accuracy: 0.9248000000\n#### Note:\n\nCIFAR-10 consists of 60,000 32x32 color images in 10 classes.\n- **Model**: ResNet18 / VGG-like models\n- **Expected Accuracy**: ~92–96%\n- **Link**: [CIFAR-10 GitHub](https://github.com/kuangliu/pytorch-cifar)\n\n> This repository explores multiple state of the art models including VGG, ResNet18, and many others providing us with a good point to start comparing the results of our implementation with further models.","metadata":{}}]}